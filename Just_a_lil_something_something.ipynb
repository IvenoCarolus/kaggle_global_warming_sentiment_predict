{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Just a lil something something.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyO5X/92PDUHL+wFSnxqVWd2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/IvenoCarolus/kaggle_global_warming_sentiment_predict/blob/master/Just_a_lil_something_something.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NDSAl2fSny6t",
        "colab_type": "text"
      },
      "source": [
        "## **Second attempt, but with a different text cleaning function.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cEc_tPB8msm3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "352fc350-4bf7-49b4-e55b-78d087b5a9ab"
      },
      "source": [
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T3SpfLbiny3t",
        "colab_type": "text"
      },
      "source": [
        "## **Geting the twitter data (train,test and sample csv files)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gEc1rtBOrLAV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sample_sub_df = pd.read_csv('https://raw.githubusercontent.com/IvenoCarolus/kaggle_global_warming_sentiment_predict/master/data/sample_submission.csv',error_bad_lines=False)\n",
        "train_df = pd.read_csv('https://raw.githubusercontent.com/IvenoCarolus/kaggle_global_warming_sentiment_predict/master/data/train.csv',error_bad_lines=False)\n",
        "test_df = pd.read_csv('https://raw.githubusercontent.com/IvenoCarolus/kaggle_global_warming_sentiment_predict/master/data/test.csv',error_bad_lines=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X5jEylQStEgY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "56f77327-0d77-4d00-c5aa-5c6576a80f4f"
      },
      "source": [
        "train_df.head()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentiment</th>\n",
              "      <th>message</th>\n",
              "      <th>tweetid</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>PolySciMajor EPA chief doesn't think carbon di...</td>\n",
              "      <td>625221</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>It's not like we lack evidence of anthropogeni...</td>\n",
              "      <td>126103</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>RT @RawStory: Researchers say we have three ye...</td>\n",
              "      <td>698562</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>#TodayinMaker# WIRED : 2016 was a pivotal year...</td>\n",
              "      <td>573736</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>RT @SoyNovioDeTodas: It's 2016, and a racist, ...</td>\n",
              "      <td>466954</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   sentiment                                            message  tweetid\n",
              "0          1  PolySciMajor EPA chief doesn't think carbon di...   625221\n",
              "1          1  It's not like we lack evidence of anthropogeni...   126103\n",
              "2          2  RT @RawStory: Researchers say we have three ye...   698562\n",
              "3          1  #TodayinMaker# WIRED : 2016 was a pivotal year...   573736\n",
              "4          1  RT @SoyNovioDeTodas: It's 2016, and a racist, ...   466954"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gX9P5e4-t8UA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "823e7cc0-6d57-4f50-b120-486c84a2b51c"
      },
      "source": [
        "test_df.head()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>message</th>\n",
              "      <th>tweetid</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Europe will now be looking to China to make su...</td>\n",
              "      <td>169760</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Combine this with the polling of staffers re c...</td>\n",
              "      <td>35326</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>The scary, unimpeachable evidence that climate...</td>\n",
              "      <td>224985</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>@Karoli @morgfair @OsborneInk @dailykos \\nPuti...</td>\n",
              "      <td>476263</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>RT @FakeWillMoore: 'Female orgasms cause globa...</td>\n",
              "      <td>872928</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                             message  tweetid\n",
              "0  Europe will now be looking to China to make su...   169760\n",
              "1  Combine this with the polling of staffers re c...    35326\n",
              "2  The scary, unimpeachable evidence that climate...   224985\n",
              "3  @Karoli @morgfair @OsborneInk @dailykos \\nPuti...   476263\n",
              "4  RT @FakeWillMoore: 'Female orgasms cause globa...   872928"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aFtpGRjRuZ0x",
        "colab_type": "text"
      },
      "source": [
        "## **Cleaning up the tweet data from the message column**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W2tj_sIbt_ae",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "corpus = []\n",
        "for tweet in train_df['message'].values:\n",
        "  #Text cleaning\n",
        "  cleaned_tweet = re.sub(r'\\W', ' ', str(tweet)) #removing all special characters\n",
        "  cleaned_tweet = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', cleaned_tweet) #removing single characters\n",
        "  cleaned_tweet = re.sub(r'\\^[a-zA-Z]\\s+', ' ', cleaned_tweet)  #removing unnecessary spaces\n",
        "  cleaned_tweet = re.sub(r'^b\\s+', '', cleaned_tweet)           #removing substring prefixed b\n",
        "  cleaned_tweet = cleaned_tweet.lower()\n",
        "  cleaned_tweet = cleaned_tweet.split()\n",
        "  all_stopwords = stopwords.words('english')\n",
        "  #cleaned_tweet = [stemmer.lemmatize(word) for word in cleaned_tweet] #does the same thing as PorterStemmer kinda,thus the performance doesnt really change it seems\n",
        "  ps = PorterStemmer()\n",
        "  cleaned_tweet = [ps.stem(word) for word in cleaned_tweet if not word in set(all_stopwords)] #stemming and making sure all stopwords from the english set is not included in our set\n",
        "  #cleaned_tweet = [stemmer.lemmatize(word) for word in cleaned_tweet if not word in set(all_stopwords)]\n",
        "  cleaned_tweet = ' '.join(cleaned_tweet)\n",
        "  corpus.append(cleaned_tweet)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mt58xH6LH8tW",
        "colab_type": "text"
      },
      "source": [
        "## **Text-to-Sequence process**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Ry6SxNNLROm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vectorizer = TfidfVectorizer()\n",
        "X = vectorizer.fit_transform(corpus).toarray()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mOhbt1INM30t",
        "colab_type": "text"
      },
      "source": [
        "## **Train Test splitting**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BNJo3ZU5NISI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y = np.array(train_df['sentiment'])\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OyKSJtOhNpoZ",
        "colab_type": "text"
      },
      "source": [
        "## **Fitting model with train data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NKolTwQdNoH1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "13d09b79-a3d7-4a96-9519-8e775f482538"
      },
      "source": [
        "classifier = LogisticRegression(C=20)\n",
        "classifier.fit(X_train, y_train)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(C=20, class_weight=None, dual=False, fit_intercept=True,\n",
              "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
              "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
              "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
              "                   warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lmsLxb1kOMO6",
        "colab_type": "text"
      },
      "source": [
        "## **Classification report**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rr7sTGyoQSXn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "3efae6f6-ffff-40f9-e875-9ee9a99cb2c2"
      },
      "source": [
        "print(classification_report(y_test, classifier.predict(X_test)))"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "          -1       0.73      0.45      0.56       249\n",
            "           0       0.57      0.41      0.48       473\n",
            "           1       0.75      0.86      0.80      1701\n",
            "           2       0.76      0.76      0.76       741\n",
            "\n",
            "    accuracy                           0.73      3164\n",
            "   macro avg       0.70      0.62      0.65      3164\n",
            "weighted avg       0.73      0.73      0.72      3164\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}